{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPXmyvw4h0wcXuB2nsESck+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RoseVZ/Papers_to_code/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On colab since I got the student Pro :))\n",
        "Free Compute Yayyy!"
      ],
      "metadata": {
        "id": "KoBgxUy5LrPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "FVXGrE0WCpwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WkW-IhS2CsAt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34OwauxpC6Af",
        "outputId": "421b3bfd-3d10-4773-fd36-cafef3a0eec8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0.1 Dataset"
      ],
      "metadata": {
        "id": "vGaS9JDzDEKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "    return s.strip()"
      ],
      "metadata": {
        "id": "ye-T9PUuDf5_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a small subset of sentence pairs\n",
        "def load_sentence_pairs(path=\"fra.txt\", max_sentences=10000):\n",
        "    pairs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            eng, fra, *_ = line.strip().split(\"\\t\")\n",
        "            eng = preprocess(eng)\n",
        "            fra = preprocess(fra)\n",
        "            pairs.append((eng, fra))\n",
        "            if len(pairs) >= max_sentences:\n",
        "                break\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "HDpXa5yUDGi2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_sentence_pairs()[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbkAUjGxDZFT",
        "outputId": "e10dad03-7a41-44c5-fdcb-0777ce2f9d6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('go .', 'va !'),\n",
              " ('go .', 'marche .'),\n",
              " ('go .', 'en route !'),\n",
              " ('go .', 'bouge !'),\n",
              " ('hi .', 'salut !')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Purpose of Vocab Class\n",
        "###The Vocab class takes in a list of sentences and builds:\n",
        "\n",
        "A word-to-index mapping: stoi (string → integer)\n",
        "\n",
        "An index-to-word mapping: itos (integer → string)\n",
        "\n",
        "Plus some special tokens like <pad\\>, <sos\\>, etc."
      ],
      "metadata": {
        "id": "vh_dxF7bD9eJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tokens are essential:\n",
        "\n",
        "<pad\\>: for padding short sequences to equal length\n",
        "\n",
        "<sos\\>: start-of-sentence (decoder input)\n",
        "\n",
        "<eos\\>: end-of-sentence (decoder supervision)\n",
        "\n",
        "<unk\\>: unknown word (out-of-vocabulary)"
      ],
      "metadata": {
        "id": "LJMEoLipEdCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer and vocabulary\n",
        "class Vocab:\n",
        "\n",
        "    def __init__(self, sentences: List[str], min_freq=2):\n",
        "        self.pad_token = \"<pad>\"\n",
        "        self.sos_token = \"<sos>\"\n",
        "        self.eos_token = \"<eos>\"\n",
        "        self.unk_token = \"<unk>\"\n",
        "\n",
        "        #Create a list of these 4 special tokens to always include at the beginning of the vocabulary.\n",
        "        special_tokens = [self.pad_token, self.sos_token, self.eos_token, self.unk_token]\n",
        "\n",
        "        #put/flatten all words into a single 1d list\n",
        "        words = [word for s in sentences for word in s.split()]\n",
        "        #Calculate the frequency of the word in the vocabulary\n",
        "        freq = Counter(words)\n",
        "\n",
        "        #store the words with frequency greater than the minimum (here 2)\n",
        "        vocab = [w for w, f in freq.items() if f >= min_freq]\n",
        "\n",
        "        #mapping from int to string including the special tokens and all the words in vocab\n",
        "        self.itos = special_tokens + sorted(vocab)\n",
        "        #reverse map: string → index\n",
        "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
        "\n",
        "    def encode(self, sentence: str) -> List[int]:\n",
        "        #first convert the sentence to a 1D list-> look up the values of each token on the string to int map(stoi)\n",
        "        #if not found return the <unk> token\n",
        "        return [self.stoi.get(w, self.stoi[self.unk_token]) for w in sentence.split()]\n",
        "\n",
        "    def decode(self, indices: List[int]) -> str:\n",
        "        #same as encode but takes a list of indices as converts to words/tokens\n",
        "        #eg. [8,4,5]->\"I love pasta\" :) [I do]\n",
        "        return \" \".join([self.itos[i] for i in indices if self.itos[i] not in [self.pad_token, self.sos_token, self.eos_token]])\n",
        "\n",
        "    def __len__(self):\n",
        "      #send the size of vocab: helpful while building NN\n",
        "        return len(self.itos)\n",
        "\n"
      ],
      "metadata": {
        "id": "-63XfkcID9PD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the eng-french pair\n",
        "pairs = load_sentence_pairs()\n",
        "#separate them\n",
        "eng_sentences = [p[0] for p in pairs]\n",
        "fra_sentences = [p[1] for p in pairs]\n",
        "\n",
        "#create the Vocab\n",
        "eng_vocab = Vocab(eng_sentences)\n",
        "fra_vocab = Vocab(fra_sentences)\n",
        "\n",
        "print(f\"English vocab size: {len(eng_vocab)}, French vocab size: {len(fra_vocab)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GLSbwyZEFR6",
        "outputId": "37d1b9d2-fbd4-49e3-81dc-daec41ed0f13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 1404, French vocab size: 1940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset prep :("
      ],
      "metadata": {
        "id": "lUi0TFdkJIUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining a size for each input sentence\n",
        "MAX_LEN = 20\n",
        "\n",
        "#pad using <pad> the i/p sentence if its shorter or truncate if longer\n",
        "def pad_seq(seq, max_len, pad_idx):\n",
        "    return seq[:max_len] + [pad_idx] * (max_len - len(seq))\n",
        "\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs: List[Tuple[str, str]], src_vocab: Vocab, tgt_vocab: Vocab):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "              pairs: list of tuples (source_sentence, target_sentence) both as strings\n",
        "              src_vocab: vocabulary object for source language (e.g., English)\n",
        "              tgt_vocab: vocabulary object for target language (e.g., French).\n",
        "        \"\"\"\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.data = []\n",
        "        for src, tgt in pairs:\n",
        "            #pad the encoded english (source vals)\n",
        "            src_ids = pad_seq(src_vocab.encode(src), MAX_LEN, src_vocab.stoi[src_vocab.pad_token])\n",
        "            #convert target = <strt> token + target_vals+ <endofsentence> token\n",
        "            tgt_ids = [tgt_vocab.stoi[tgt_vocab.sos_token]] + tgt_vocab.encode(tgt) + [tgt_vocab.stoi[tgt_vocab.eos_token]]\n",
        "            #pad to MAX_LEN+2 for sos and eos tokens\n",
        "            tgt_ids = pad_seq(tgt_ids, MAX_LEN + 2, tgt_vocab.stoi[tgt_vocab.pad_token])\n",
        "            #append the src and target to data\n",
        "            self.data.append((src_ids, tgt_ids))\n",
        "\n",
        "    def __len__(self):\n",
        "      #return len of data\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      #Given an index idx, returns the source and target sequences as PyTorch tensors.\n",
        "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n"
      ],
      "metadata": {
        "id": "w6WoO9H9JIIp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TranslationDataset(pairs, eng_vocab, fra_vocab)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "cEObv9DxI8s6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Building"
      ],
      "metadata": {
        "id": "vt70VTiiLm7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Blocks\n"
      ],
      "metadata": {
        "id": "D2yy-FErX2Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        d_model: embedding dimension\n",
        "        max_len: maximum length of the input sequence\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        #initializing a matrix of zeros for positonal embeddings with shape (max_len)x embedding_dim\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        #Creates a column vector [0, 1, ..., max_len-1] to represent positions with shape max_lenx1\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "\n",
        "        #Computes a scaling term that varies across dimensions.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        #each position gets a unique encoding.\n",
        "        #even gets sin and odd gets cosine\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model) adds batch dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "      #Adds positional encodings to the input embeddings.\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n"
      ],
      "metadata": {
        "id": "bamWpsUVLhXR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Multi-head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            d_model: embedding dimension\n",
        "            n_heads: number of attention heads\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Number of heads must evenly divide the embedding dimension\n",
        "        assert d_model % n_heads == 0\n",
        "        # Dimension of each head\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Linear layers to project inputs to queries, keys and values\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        # Final linear layer after concatenating heads\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # Get batch size and sequence lengths of q, k, v and embedding dims\n",
        "        B, Lq, Dq = q.size()\n",
        "        B, Lk, Dk = k.size()\n",
        "        B, Lv, Dv = v.size()\n",
        "\n",
        "        # Project queries, keys, values and reshape to (B, n_heads, Seq_len, d_k)\n",
        "        q = self.q_linear(q).view(B, Lq, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_linear(k).view(B, Lk, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_linear(v).view(B, Lv, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention scores\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        # Apply mask if provided (e.g., padding or future tokens)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "        # Normalize attention scores to probabilities\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        # Compute weighted sum of values according to attention\n",
        "        context = torch.matmul(attn, v)\n",
        "        # Concatenate all heads and reshape back to (B, Lq, Dq)\n",
        "        context = context.transpose(1, 2).contiguous().view(B, Lq, Dq)\n",
        "\n",
        "        # Final linear projection\n",
        "        return self.out(context)"
      ],
      "metadata": {
        "id": "EjefXhXIL-V3"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            #expands to size d_ff\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            #reduces back to d_model\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "t7pWl6jYSzTi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        #Self-attention + Feed-forward sublayer.\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "\n",
        "        #two normalization layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "      #Self-attention + residual + norm\n",
        "        x2 = self.norm1(x + self.attn(x, x, x, mask))\n",
        "\n",
        "      # Feed-forward + residual + norm\n",
        "        x3 = self.norm2(x2 + self.ff(x2))\n",
        "        return x3\n",
        "\n"
      ],
      "metadata": {
        "id": "fIQaqxl9UFHb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        #Self-attention for target tokens and attention over encoder output\n",
        "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.enc_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, enc_out, tgt_mask=None, src_mask=None):\n",
        "      #Masked self-attention on decoder inputs\n",
        "        x = self.norm1(x + self.self_attn(x, x, x, tgt_mask))\n",
        "      #Encoder-decoder attention (cross-attn)\n",
        "        x = self.norm2(x + self.enc_attn(x, enc_out, enc_out, src_mask))\n",
        "        #Feed-forward\n",
        "        x = self.norm3(x + self.ff(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "DeYR9byPUGlh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assembling the blocks"
      ],
      "metadata": {
        "id": "jm5jCQMgX5Fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_len):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            vocab_size: size of the input\n",
        "            d_model: embedding dimension\n",
        "            n_heads: number of attention heads\n",
        "            d_ff: dimension of the feed-forward network\n",
        "            num_layers: number of encoder layers\n",
        "            max_len: maximum length of the input sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #embedding lookup table: maps word indices to d_model-dimensional vectors.\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        #positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        #a list of encoder layers (self-attention + feedforward blocks)\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "        #prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "      #Embeds the input and scales it by √d_model\n",
        "\n",
        "        x = self.embedding(src) * math.sqrt(self.embedding.embedding_dim)\n",
        "        print(\"Embedding output shape:\", x.shape)\n",
        "        #gets positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        #creates encoding layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "3lOy30gaX7pi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, num_layers, max_len):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            vocab_size: size of the input\n",
        "            d_model: embedding dimension\n",
        "            n_heads: number of attention heads\n",
        "            d_ff: dimension of the feed-forward network\n",
        "            num_layers: number of decoder layers\n",
        "            max_len: maximum length of the input sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #Embeds target token indices\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        #adds positional info\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        #list of decoding layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "        #avoid overfitting\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, tgt, enc_out, tgt_mask=None, src_mask=None):\n",
        "      #Embedding + scaling + positional encoding + dropout.\n",
        "        x = self.embedding(tgt) * math.sqrt(self.embedding.embedding_dim)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, enc_out, tgt_mask, src_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "G3DZRhibX_De"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, n_heads=8,\n",
        "                 d_ff=2048, num_layers=6, max_len=100):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            src_vocab_size: size of the source vocabulary\n",
        "            tgt_vocab_size: size of the target vocabulary\n",
        "            d_model: embedding dimension\n",
        "            n_heads: number of attention heads\n",
        "            d_ff: dimension of the feed-forward network\n",
        "            num_layers: number of encoder and decoder layers\n",
        "            max_len: maximum length of the input sequence\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        #encoder\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, n_heads, d_ff, num_layers, max_len)\n",
        "        #decoder\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, n_heads, d_ff, num_layers, max_len)\n",
        "        #fully connected feed forward network\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        enc_out = self.encoder(src, src_mask)\n",
        "        dec_out = self.decoder(tgt, enc_out, tgt_mask, src_mask)\n",
        "        return self.fc_out(dec_out)"
      ],
      "metadata": {
        "id": "fExJNhY8YAZu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### some sort of Masking to prevent transformers from attending to padding and other stuff"
      ],
      "metadata": {
        "id": "nHdukLTVZf43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pad_mask(matrix, pad_token=0):\n",
        "  #Returns a boolean mask where True indicates non-padding positions.\n",
        "    return (matrix != pad_token).unsqueeze(1).unsqueeze(2)"
      ],
      "metadata": {
        "id": "x7DlOYA_ZzHn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size, device):\n",
        "    # Create upper triangular matrix of shape (1,1,size,size) on the given device\n",
        "    mask = torch.triu(torch.ones((1, 1, size, size), device=device), diagonal=1).bool()\n",
        "    # Invert mask: True means allowed, False means masked (future tokens)\n",
        "    return ~mask  # shape: (1, 1, size, size)"
      ],
      "metadata": {
        "id": "s5HoAfKzZ4oM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_masks(tgt_seq, pad_token=0):\n",
        "    pad_mask = create_pad_mask(tgt_seq, pad_token)  # shape: (B, 1, 1, T)\n",
        "    look_ahead_mask = create_look_ahead_mask(tgt_seq.size(1)).to(tgt_seq.device)  # shape: (1, 1, T, T)\n",
        "    return pad_mask & ~look_ahead_mask  # valid tokens"
      ],
      "metadata": {
        "id": "Tig-3H4iZ8Fj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "uUiMKO8qaBy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss_fn(pred, target, pad_token=0):\n",
        "   # padding tokens don’t contribute to loss.\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token)\n",
        "    return loss_fn(pred.view(-1, pred.size(-1)), target.view(-1))"
      ],
      "metadata": {
        "id": "o4uWsBP5Z-48"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = Vocab([pair[0] for pair in pairs])\n",
        "tgt_vocab = Vocab([pair[1] for pair in pairs])"
      ],
      "metadata": {
        "id": "jvPpMD6ta0Pp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)"
      ],
      "metadata": {
        "id": "AP47fABZazVs"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model=256, n_heads=4, d_ff=512, num_layers=2, max_len=100).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n"
      ],
      "metadata": {
        "id": "Nl_8w0THaRZ5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src_batch, tgt_batch in train_loader:\n",
        "        src_batch = src_batch.to(device)\n",
        "        tgt_batch = tgt_batch.to(device)\n",
        "\n",
        "        tgt_input = tgt_batch[:, :-1]\n",
        "        tgt_output = tgt_batch[:, 1:]\n",
        "\n",
        "        src_mask = create_pad_mask(src_batch, src_vocab.stoi[src_vocab.pad_token]).to(device)\n",
        "        tgt_mask = combine_masks(tgt_input, tgt_vocab.stoi[tgt_vocab.pad_token])\n",
        "\n",
        "        preds = model(src_batch, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)\n",
        "        loss = masked_loss_fn(preds, tgt_output, tgt_vocab.stoi[tgt_vocab.pad_token])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boLC6lYBadTq",
        "outputId": "b22eab55-ab1c-47e7-c799-479a1eb1a545"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.2369\n",
            "Epoch 2/10, Loss: 1.1476\n",
            "Epoch 3/10, Loss: 1.0621\n",
            "Epoch 4/10, Loss: 0.9905\n",
            "Epoch 5/10, Loss: 0.9192\n",
            "Epoch 6/10, Loss: 0.8568\n",
            "Epoch 7/10, Loss: 0.8010\n",
            "Epoch 8/10, Loss: 0.7515\n",
            "Epoch 9/10, Loss: 0.7023\n",
            "Epoch 10/10, Loss: 0.6605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INFERENCE"
      ],
      "metadata": {
        "id": "M5STAvevs7gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def translate_sentence(model, src_sentence, src_vocab, tgt_vocab, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess and encode the source sentence\n",
        "    src_tokens = src_vocab.encode(src_sentence)\n",
        "    src_tokens = src_tokens[:max_len]  # truncate if longer than max_len\n",
        "    src_tokens += [src_vocab.stoi[src_vocab.pad_token]] * (max_len - len(src_tokens))  # pad\n",
        "\n",
        "    src_tensor = torch.tensor([src_tokens], dtype=torch.long).to(device)  # (1, max_len)\n",
        "\n",
        "    # Create source mask (pad tokens masked)\n",
        "    src_mask = (src_tensor != src_vocab.stoi[src_vocab.pad_token]).unsqueeze(1).unsqueeze(2)  # (1,1,1,max_len)\n",
        "\n",
        "    # Start with <sos> token as initial target input\n",
        "    tgt_tokens = [tgt_vocab.stoi[tgt_vocab.sos_token]]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor([tgt_tokens], dtype=torch.long).to(device)  # (1, current_len)\n",
        "\n",
        "        # Create target mask (pad + look-ahead mask)\n",
        "        tgt_mask = create_look_ahead_mask(tgt_tensor.size(1), device).to(device)\n",
        "        tgt_pad_mask = (tgt_tensor != tgt_vocab.stoi[tgt_vocab.pad_token]).unsqueeze(1).unsqueeze(2).to(device)\n",
        "        combined_tgt_mask = tgt_pad_mask & tgt_mask  # (1,1,len,len)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            outputs = model(src_tensor, tgt_tensor, src_mask=src_mask, tgt_mask=combined_tgt_mask)\n",
        "\n",
        "        # Get probabilities of the last token\n",
        "        last_token_logits = outputs[0, -1, :]  # (vocab_size,)\n",
        "        predicted_id = torch.argmax(last_token_logits).item()\n",
        "\n",
        "        # Stop if <eos> predicted\n",
        "        if predicted_id == tgt_vocab.stoi[tgt_vocab.eos_token]:\n",
        "            break\n",
        "\n",
        "        tgt_tokens.append(predicted_id)\n",
        "\n",
        "    # Decode to words, skip special tokens\n",
        "    translated_sentence = tgt_vocab.decode(tgt_tokens[1:])  # remove <sos> token\n",
        "\n",
        "    return translated_sentence"
      ],
      "metadata": {
        "id": "EWTFftVRs7OE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"go .\"\n",
        "translation = translate_sentence(model, sentence, eng_vocab, fra_vocab, max_len=20, device=device)\n",
        "print(\"English:\", sentence)\n",
        "print(\"French:\", translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF_9a4zPs90g",
        "outputId": "b763225c-ec43-4c81-a3a1-021fe6a86c61"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: go .\n",
            "French: va !\n"
          ]
        }
      ]
    }
  ]
}